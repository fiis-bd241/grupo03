# Próximos Pasos

## Adición de nuevas funcionalidades a la aplicación




## Subir la aplicación al servidor

### Despliegue automático con la CLI

Una de las opciones que te brinda angular para desplegar la aplicación es el "despliegue automático con la CLI" mediante el comando ``` ng deploy ```, el cual ejecuta el constructor de despliegue asociado con el proyecto.

Estos despliegues pueden ser realizados en diferentes constructores de diferentes plataformas, puedes agregar cualquiera de ellos al proyecto ejecutando ``` ng add [nombre paquete] ``` .

Cuando agregues el paquete con capacidad de despliegue, se actualizará automáticamente tu configuración de espacio de trabajo con una sección de despliegue para el proyectos seleccionado. Luego se puede utilizar el comando ``` ng deploy ``` para desplegar el proyecto.

Aquí hay una lista de los paquetes que implementan funcionalidad de despliegue en diferentes plataformas:

| Plataforma | Paquete |
|------------|---------|
| Firebase | @angular/fire |
| Azure | @azure/ng-deploy |
| Now | @zeit/ng-deploy |
| Netlify  | @netlify-builder/deploy |
| GitHub pages | angular-cli-ghpages |
|NPM |ngx-deploy-npm |
|Amazon Cloud S3 | @jefiozie/ngx-aws-deploy|



## Implementación de IA en el servidor

Se podría plantear la instalación de un chatbot basado en una API de ChatGPT o Llama 3 entrenada con los lineamientos de la empresa para apoyar a los trabajadores en algún proceso de la migración. Para implementar un chatbot basado en ChatGPT se deben seguir los siguientes pasos:

#### 1. Configuración en Spring Boot:
Para instalar un chatbot que utilice la API de ChatGPT y Llama3 (instalado localmente con Ollama) en una aplicación que usa Angular para el frontend y Spring Boot para el backend, puedes seguir los siguientes pasos:

1. **Crear un proyecto Spring Boot:**
   - Usar Spring Initializr para crear un nuevo proyecto Spring Boot con las dependencias `Spring Web` y `Spring Boot DevTools`.

2. **Configurar las dependencias en `pom.xml`:**
   ```xml
   <dependencies>
       <dependency>
           <groupId>org.springframework.boot</groupId>
           <artifactId>spring-boot-starter-web</artifactId>
       </dependency>
       <dependency>
           <groupId>org.springframework.boot</groupId>
           <artifactId>spring-boot-devtools</artifactId>
           <scope>runtime</scope>
       </dependency>
   </dependencies>
   ```

3. **Crear controladores REST en Spring Boot:**
   - Crear un controlador para manejar las peticiones del frontend y comunicarse con las APIs de ChatGPT y Llama3.

   ```java
   @RestController
   @RequestMapping("/api/chat")
   public class ChatController {

       @PostMapping("/chatgpt")
       public ResponseEntity<String> chatWithGPT(@RequestBody ChatRequest request) {
           // Lógica para llamar a la API de ChatGPT
           String response = chatService.callChatGPT(request.getMessage());
           return ResponseEntity.ok(response);
       }

       @PostMapping("/llama3")
       public ResponseEntity<String> chatWithLlama(@RequestBody ChatRequest request) {
           // Lógica para llamar a la API de Llama3 instalada localmente
           String response = chatService.callLlama3(request.getMessage());
           return ResponseEntity.ok(response);
       }
   }

   public class ChatRequest {
       private String message;

       // Getters y setters
   }
   ```

4. **Implementar la lógica para llamar a las APIs:**
   - Usar `RestTemplate` o `WebClient` para llamar a las APIs de ChatGPT y Llama3 desde el controlador.

   ```java
   @Service
   public class ChatService {

       public String callChatGPT(String message) {
           // Implementar llamada a la API de ChatGPT
           RestTemplate restTemplate = new RestTemplate();
           String url = "https://api.openai.com/v1/engines/davinci-codex/completions";
           HttpHeaders headers = new HttpHeaders();
           headers.setContentType(MediaType.APPLICATION_JSON);
           headers.set("Authorization", "Bearer YOUR_API_KEY");
           Map<String, String> request = new HashMap<>();
           request.put("prompt", message);
           HttpEntity<Map<String, String>> entity = new HttpEntity<>(request, headers);
           ResponseEntity<String> response = restTemplate.postForEntity(url, entity, String.class);
           return response.getBody();
       }

       public String callLlama3(String message) {
           // Implementar llamada a la API de Llama3
           RestTemplate restTemplate = new RestTemplate();
           String url = "http://localhost:11434/api/generate";
           HttpHeaders headers = new HttpHeaders();
           headers.setContentType(MediaType.APPLICATION_JSON);
           Map<String, Object> request = new HashMap<>();
           request.put("model", "llama3");
           request.put("prompt", message);
           HttpEntity<Map<String, Object>> entity = new HttpEntity<>(request, headers);
           ResponseEntity<String> response = restTemplate.postForEntity(url, entity, String.class);
           return response.getBody();
       }
   }
   ```

#### Paso 2: Configuración de Angular

1. **Crear un proyecto Angular:**
   - Usar Angular CLI para crear un nuevo proyecto Angular.
   ```bash
   ng new chat-app
   cd chat-app
   ```

2. **Crear un servicio Angular para comunicarse con el backend:**
   - Crear un servicio para manejar las peticiones HTTP hacia el backend de Spring Boot.

   ```typescript
   import { Injectable } from '@angular/core';
   import { HttpClient } from '@angular/common/http';
   import { Observable } from 'rxjs';

   @Injectable({
     providedIn: 'root'
   })
   export class ChatService {
     private apiUrl = 'http://localhost:8080/api/chat';

     constructor(private http: HttpClient) {}

     chatWithGPT(message: string): Observable<string> {
       return this.http.post<string>(`${this.apiUrl}/chatgpt`, { message });
     }

     chatWithLlama(message: string): Observable<string> {
       return this.http.post<string>(`${this.apiUrl}/llama3`, { message });
     }
   }
   ```

3. **Crear componentes Angular para el chatbot:**
   - Crear un componente para la interfaz del chatbot.

   ```typescript
   import { Component } from '@angular/core';
   import { ChatService } from './chat.service';

   @Component({
     selector: 'app-chat',
     template: `
       <div>
         <h1>Chatbot</h1>
         <input [(ngModel)]="message" placeholder="Type your message"/>
         <button (click)="sendMessage()">Send</button>
         <div *ngIf="response">
           <p>{{ response }}</p>
         </div>
       </div>
     `
   })
   export class ChatComponent {
     message: string = '';
     response: string | null = null;

     constructor(private chatService: ChatService) {}

     sendMessage() {
       this.chatService.chatWithGPT(this.message).subscribe(res => {
         this.response = res;
       });
     }
   }
   ```

4. **Agregar el componente al módulo principal de Angular:**
   ```typescript
   import { NgModule } from '@angular/core';
   import { BrowserModule } from '@angular/platform-browser';
   import { FormsModule } from '@angular/forms';
   import { HttpClientModule } from '@angular/common/http';
   import { AppComponent } from './app.component';
   import { ChatComponent } from './chat/chat.component';

   @NgModule({
     declarations: [
       AppComponent,
       ChatComponent
     ],
     imports: [
       BrowserModule,
       FormsModule,
       HttpClientModule
     ],
     providers: [],
     bootstrap: [AppComponent]
   })
   export class AppModule { }
   ```

### Paso 3: Configuración de Llama3 con Ollama
Ollama es una plataforma que facilita la implementación y gestión de modelos de inteligencia artificial y machine learning de forma local. Una de estas opciones es el Modelo de Lenguaje Grande (LLM) Llama 3 desarrollado por Meta, que es una opción alternativa al uso de ChatGPT.

Para implementar un chatbot basado en Llama 3 se deben seguir los siguientes pasos:

1. **Descargar e instalar Ollama:**
   - La descarga se puede hacer desde [aquí](https://ollama.com/download).

2. **Configurar y correr el servidor de Llama3 con Ollama:**
   - Para correr un servidor con el modelo Llama3 se usan los comandos de ollama en el cmd:
     ```bash
     ollama run llama3:70b
     ```

3. **Usar la API de Ollama:**
   - La documentación de la API está disponible en [GitHub](https://github.com/ollama/ollama/blob/main/docs/api.md).

### Ejemplo de llamada a la API de Llama3 usando Ollama:

```java
public String callLlama3(String message) {
    RestTemplate restTemplate = new RestTemplate();
    String url = "http://localhost:11434/api/generate";
    HttpHeaders headers = new HttpHeaders();
    headers.setContentType(MediaType.APPLICATION_JSON);
    Map<String, Object> request = new HashMap<>();
    request.put("model", "llama3");
    request.put("prompt", message);
    HttpEntity<Map<String, Object>> entity = new HttpEntity<>(request, headers);
    ResponseEntity<String> response = restTemplate.postForEntity(url, entity, String.class);
    return response.getBody();
}
```

### Conclusión:
Siguiendo estos pasos, podrás configurar un chatbot que utilice tanto la API de ChatGPT como Llama3 instalado localmente en una aplicación de Angular y Spring Boot. Esto proporcionará una experiencia de chatbot integrada y robusta en tu aplicación.

## RoadMap

<div>
<img src=".\resources\Roadmap.png" alt="RoadMap" style="width: auto; height: auto;"/>
</div>


[Regresar al índice](Indice.md)
